{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from scipy.stats import entropy as entr\n",
    "from numpy.linalg import norm\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from time import time\n",
    "from time import sleep\n",
    "\n",
    "from pandas_datareader import data, wb\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "import seaborn as sns\n",
    "from  matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def formatPythonList(string):\n",
    "    return  [t.replace(\"{\",\"\").replace(\"}\",\"\").replace(\",\",\"\").replace(\"*^\",\"e\") \n",
    "             for t in string.fields()[0]]\n",
    "\n",
    "\n",
    "def mathematicaMaxEnt(list1, list2, norm=1):\n",
    "    l1 = \"List\"+str(list1)\n",
    "    l2 = \"List\"+str(list2)\n",
    "\n",
    "    maxent = !./maxent.sh \"$l1\" \"$l2\" \"$norm\"\n",
    "    maxent = formatPythonList(maxent)\n",
    "\n",
    "    return np.array(map(np.float, maxent)).reshape(len(list1), len(list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def JSD(P, Q):\n",
    "    p = np.array(P)\n",
    "    q = np.array(Q)\n",
    "    if len(p.shape) > 1:\n",
    "        p = p.flatten()\n",
    "        q = q.flatten()\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5/np.log(2.) * (entr(p, m) + entr(q, m))\n",
    "\n",
    "\n",
    "def KLD(P, Q):\n",
    "    p = np.array(P)\n",
    "    q = np.array(Q)\n",
    "    if len(p.shape) > 1:\n",
    "        p = p.flatten()\n",
    "        q = q.flatten()\n",
    "    return entr(p, q)\n",
    "\n",
    "\n",
    "def S_KLD(X, Y, t0=None, t1=None):\n",
    "    x = X.loc[t0:t1]\n",
    "    y = Y.loc[t0:t1]\n",
    "    \n",
    "    x_bins = x.value_counts(True, True, True).sort_index().values.tolist()\n",
    "    y_bins = y.value_counts(True, True, True).sort_index().values.tolist()\n",
    "    \n",
    "    maxent_distr = mathematicaMaxEnt(x_bins, y_bins) \n",
    "    emp_distr = pd.crosstab(x, y, normalize=True)\n",
    "    \n",
    "    return KLD(emp_distr, maxent_distr)\n",
    "\n",
    "\n",
    "def rescaleRange(x):\n",
    "    return 1 - np.exp(-2.*x)\n",
    "\n",
    "\n",
    "def stringCSV(name, *arg):\n",
    "    string = name\n",
    "    if len(arg) > 0:\n",
    "        string = string + \"_\"\n",
    "        arg = [a for a in arg if a is not None]\n",
    "        arg = map(str, arg)\n",
    "        \n",
    "    return string + \"_\".join(arg) + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read returns, downloaded and preprocessed in corresponding file\n",
    "df = pd.read_csv(\"data/M10_2005_2018_returns_filtered.csv\", \n",
    "                 engine='c', index_col='time', parse_dates=True)\n",
    "\n",
    "# Symbols to potentially analyze\n",
    "symbols = [\"EUR_AUD\", \"EUR_CAD\", \"EUR_CHF\", \"EUR_EUR\", \"EUR_GBP\", \"EUR_JPY\", \n",
    "           \"EUR_MXN\", \"EUR_NOK\", \"EUR_NZD\", \"EUR_SEK\", \"EUR_USD\", \"EUR_ZAR\"]\n",
    "curr = [sym[-3:] for sym in symbols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class Currency Set. \n",
    "# Each instance comprises a set of select currencies during a certain time frame.\n",
    "# Since part of the analysis is ranking currency returns, specifying the subset matters.\n",
    "class currencySet(object):\n",
    "    \n",
    "    def __init__(self, df, symbols=None, t0=None, t1=None):\n",
    "        \n",
    "        if t0 == None:\n",
    "            t0 = \"20080101\"\n",
    "        \n",
    "        # Read the files of returns\n",
    "        self._returns = df.loc[t0:t1, symbols]\n",
    "        self._no = len(self._returns.columns)\n",
    "\n",
    "        # Ranking of returns and rescaling\n",
    "        self._ranks = self._returns.rank(axis=1, numeric_only=True, method='first')\n",
    "        self._ranks = 2*self._ranks.applymap(np.int) - (self._no+1)\n",
    "\n",
    "        self._curr = [_c[4:] for _c in self._ranks.columns]\n",
    "        self._ranks.columns = self._curr\n",
    "        \n",
    "        self._kld_df = None\n",
    "        \n",
    "        self._bincounts = self._ranks.apply(pd.Series.value_counts, args=(True,True,True,))\n",
    "\n",
    "        \n",
    "    # Routine to calculate the Kullback-Leibler divergence. \n",
    "    # Requires scipy.stats entropy routine.\n",
    "    def _KLD(self, P, Q):\n",
    "        p = np.array(P)\n",
    "        q = np.array(Q)\n",
    "        if len(p.shape) > 1:\n",
    "            p = p.flatten()\n",
    "            q = q.flatten()\n",
    "        return entr(p, q)\n",
    "\n",
    "\n",
    "    # Calculate Kullback-Leibler divergence between empirical distribution and the \n",
    "    # MaxEnt distribution from the marginales.\n",
    "    # If we have a ranking, then rank_i != rank_j needs to be enforced.\n",
    "    # Refer to mathematicaMaxEnt for this. \n",
    "    # Otherwise, np.outer suffices.\n",
    "    def _S_KLD(self, X, Y, maxent_func=mathematicaMaxEnt, t0=None, t1=None):\n",
    "        x = X.loc[t0:t1]\n",
    "        y = Y.loc[t0:t1]\n",
    "\n",
    "        x_bins = x.value_counts(True, True, True).sort_index().values.tolist()\n",
    "        y_bins = y.value_counts(True, True, True).sort_index().values.tolist()\n",
    "\n",
    "        maxent_distr = maxent_func(x_bins, y_bins) \n",
    "        emp_distr = pd.crosstab(x, y, normalize=True).as_matrix()\n",
    "        \n",
    "        return self._KLD(emp_distr, maxent_distr)\n",
    "    \n",
    "    \n",
    "    # Calculate S_KLD for all currency pairs and return the resulting matrix.\n",
    "    def _S_KLD_matrix(self, df, no, lag):\n",
    "        curr = self._curr\n",
    "        # If no lag, then calculate for i, j, and i!=j\n",
    "        # Else, calculate all i, j pairs, but use symmetry.\n",
    "        # To do that, create matrix where all pairs to be calculated are np.nan\n",
    "        if lag > 0:\n",
    "            kld_df = pd.DataFrame(data=np.empty((no,no,)).fill(False),\n",
    "                                  index=[c+ \"_predictor\" for c in curr], columns=curr)\n",
    "            maxent_func = np.outer\n",
    "        else:\n",
    "            kld_df = pd.DataFrame(data=np.identity(no),\n",
    "                                  index=curr, columns=curr).replace(0, False) - 1\n",
    "            maxent_func = mathematicaMaxEnt\n",
    "        \n",
    "        for i in range(no):\n",
    "            for j in range(no):\n",
    "                if kld_df.iloc[i,j]:\n",
    "                    df_i, df_j = self._predictor(df.iloc[:, [i,j]], lag) \n",
    "                    kld_df.iloc[i,j] = rescaleRange(self._S_KLD(df_i, df_j, maxent_func))\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                if lag == 0:\n",
    "                    kld_df.iloc[j,i] = kld_df.iloc[i,j]\n",
    "        \n",
    "        return kld_df\n",
    "\n",
    "    \n",
    "    # Instead of considering concurrent rankings, we can study lagged relationships.\n",
    "    # This routine shifts one time series and makes sure that only true 10-minute intervals\n",
    "    # are being considered in analysis.\n",
    "    # If lag 0, funtion only returns columns separately.\n",
    "    def _predictor(self, df, lag):\n",
    "        if lag > 0:\n",
    "            c1, c2 = df.columns\n",
    "            df = pd.merge(df.iloc[:, 0].shift(lag).to_frame(c1 + \"_predictor\"),\n",
    "                          df.iloc[:, 1].to_frame(c2), \n",
    "                          left_index=True, right_index=True)\n",
    "            df = df.dropna().applymap(np.int)\n",
    "            \n",
    "        return df.iloc[:, 0], df.iloc[:, 1]\n",
    "\n",
    "    \n",
    "    # Time slicing according to quarterly analysis, annual analysis\n",
    "    # or for whole time period\n",
    "    # To-do: make more intuitive and replace all the if-else.\n",
    "    def _timeslicer(self, year, q):\n",
    "        # Allow to evaluate quarterly. \n",
    "        # If no quarter is specified, select whole year\n",
    "            \n",
    "        if year != None:\n",
    "            y1, y2 = str(year), str(year)    \n",
    "        else:\n",
    "            y1, y2 = str(self._ranks.index[0].year), str(self._ranks.index[-1].year)\n",
    "            q = None\n",
    "            \n",
    "        if q != None:\n",
    "            q0, q1 = '{:02d}'.format(3*q-2), '{:02d}'.format(3*q)\n",
    "        else:\n",
    "            q0, q1 = \"01\", \"12\"\n",
    "            \n",
    "        \n",
    "        loc0 = pd.to_datetime(y1 + \"-\" + q0 + \"-01\")\n",
    "        loc1 = pd.to_datetime(y2 + \"-\" + q1) + MonthEnd(1)\n",
    "        \n",
    "        return loc0, loc1\n",
    "      \n",
    "    \n",
    "    def get_bincounts(self):\n",
    "        return self._bincounts\n",
    "    \n",
    "    \n",
    "    def get_curr(self):\n",
    "        return self._curr\n",
    "    \n",
    "        \n",
    "    def get_kld(self):\n",
    "        return self._kld\n",
    "    \n",
    "    \n",
    "    def get_ranks(self):\n",
    "        return self._ranks\n",
    "    \n",
    "    \n",
    "    def get_ranks_shifted(self, lag, dropna=True):\n",
    "        # Shift ranks by lag and return as new dataframe\n",
    "        df = self.get_ranks().resample(\"10T\").last().diff(lag)\n",
    "        if dropna:\n",
    "            df = df.dropna()\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def get_returns(self):\n",
    "        return self._returns\n",
    "    \n",
    "    \n",
    "    def make_kld(self, year=None, q=None, lag=0, export=True):\n",
    "        # Number of currencies\n",
    "        no = self._no\n",
    "        \n",
    "        loc0, loc1 = self._timeslicer(year, q)\n",
    "    \n",
    "        df = self._ranks.loc[loc0:loc1]\n",
    "                \n",
    "        # Calculate Kullback-Leibler divergence for all pairs of empirical and theoretical \n",
    "        # bivariate distributions and rescale to [0,1].\n",
    "        kld_df = self._S_KLD_matrix(df, no, lag)\n",
    "        \n",
    "        # After calculating one triangle of matrix, mirror along diagonal and \n",
    "        # set diagonal to np.nan\n",
    "        kld_df = kld_df.replace(0, np.nan)\n",
    "        \n",
    "        if export == True:\n",
    "            kld_df.to_csv(stringCSV(\"KLD\", no, year, q, \"lag_\"+str(lag)), index=True)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        self._kld = kld_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parent class to download data\n",
    "\n",
    "class macroData(object):\n",
    "    def __init__(self, cset=None, t0=None, t1=None):\n",
    "        # Variables for API url and access token\n",
    "        self._url = None\n",
    "        self._access_token = None\n",
    "        \n",
    "        self._cset = cset\n",
    "        \n",
    "        # Default state date: January 2008\n",
    "        # Default end date: now\n",
    "        # Format as strings as well as datetime\n",
    "        if t0 == None:\n",
    "            t0 = \"20080101\"\n",
    "        self._t0 = pd.to_datetime(t0, format=\"%Y%m%d\")\n",
    "        \n",
    "        if t1 == None:\n",
    "            self._t1 = pd.to_datetime(time(), unit=\"s\")\n",
    "        else:\n",
    "            self._t1 = pd.to_datetime(t1, format=\"%Y%m%d\")\n",
    "        \n",
    "        self._st0 = str(self._t0)[0:10]\n",
    "        self._st1 = str(self._t1)[0:10]\n",
    "           \n",
    "            \n",
    "        # Open lookup table for conversions between different sources\n",
    "        # Throw error if file can't be opened and read properly.\n",
    "        try:\n",
    "            self._lookup_df = pd.read_csv(\"iso_codes.csv\", index_col=0).dropna()\n",
    "            self._lookup_df = self._lookup_df.rename(columns={\"pt3ISO\":\"ISO\"})\n",
    "            self._iso_dict = self._lookup_df.dropna().set_index(\"ISO\").to_dict()\n",
    "            self._tb_dict = self._lookup_df.dropna().set_index(\"FX\").to_dict()[\"TB\"]\n",
    "        except IOError as e:\n",
    "            print \"Failing to open ISO code file.\", str(e)\n",
    "    \n",
    "    \n",
    "    # Take numeric dataframe with time as index.\n",
    "    # Convert index to datetime and data to float.\n",
    "    def _format_float_df(self, df):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df.applymap(np.float)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "        \n",
    "    # Routine to request data from API\n",
    "    def _request(self, **params):\n",
    "        # Allow for endpoint in API\n",
    "        if \"endpoint\" in params:\n",
    "            url =  \"%s/%s\" % (self._url, params[\"endpoint\"])\n",
    "        else:\n",
    "            url = self._url\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "        except requests.RequestException as e:\n",
    "            print (str(e))\n",
    "            \n",
    "        content = response.content.decode('utf-8')\n",
    "        try:\n",
    "            content = json.loads(content)\n",
    "        except ValueError as e:\n",
    "            print \"Content from API could not be decoded\", str(e)\n",
    "            print \"Returning content as is.\"\n",
    "        \n",
    "        return content  \n",
    "    \n",
    "    \n",
    "    # Classification of currencies as reserve or commodity\n",
    "    def currency_class(self):\n",
    "        c_com_dict = {'AUD': 1, 'CAD': 1, 'CHF': 0, 'EUR': 0, 'GBP': 0, 'JPY': 0, \n",
    "                      'MXN': 0, 'NOK': 1, 'NZD': 0, 'SEK': 0, 'USD': 0, 'ZAR': 1}\n",
    "        c_res_dict = {'AUD': 0, 'CAD': 0, 'CHF': 1, 'EUR': 1, 'GBP': 1, 'JPY': 1, \n",
    "                      'MXN': 0, 'NOK': 0, 'NZD': 0, 'SEK': 0, 'USD': 1, 'ZAR': 0}\n",
    "        \n",
    "        return pd.concat([\n",
    "            pd.DataFrame.from_dict(c_com_dict, orient=\"index\").rename(columns={0:\"Comm_Curr\"}),\n",
    "            pd.DataFrame.from_dict(c_res_dict, orient=\"index\").rename(columns={0:\"Reserve_Curr\"})],\n",
    "            axis=1)\n",
    "            \n",
    "        \n",
    "    # Classification of currencies according to continent / timezone.\n",
    "    def same_continent(self):\n",
    "        curr = self._cset\n",
    "        cont_dict = self._lookup_df.loc[:, [\"FX\", \"Cont\"]].set_index(\"FX\").to_dict()[\"Cont\"]\n",
    "        \n",
    "        continent_df = pd.DataFrame(index=curr, columns=curr)\n",
    "        continent_df = continent_df.unstack().reset_index().drop(0, axis=1)\n",
    "        continent_df = pd.concat([continent_df.rename(columns={\"level_0\":\"country_1\", \"level_1\":\"country_2\"}),\n",
    "                                  continent_df.rename(columns={\"level_0\":\"continent_1\", \"level_1\":\"continent_2\"})], \n",
    "                                 axis=1)\n",
    "        continent_df = continent_df.loc[continent_df[\"continent_1\"] != continent_df[\"continent_2\"]]\n",
    "        continent_df = continent_df.set_index([\"country_1\", \"country_2\"])\n",
    "        continent_df.loc[:, \"continent_1\"] = continent_df.loc[:, \"continent_1\"].map(cont_dict)\n",
    "        continent_df.loc[:, \"continent_2\"] = continent_df.loc[:, \"continent_2\"].map(cont_dict)\n",
    "        continent_df.loc[:, \"same_cont\"] = (continent_df.loc[:, \"continent_1\"]==continent_df.loc[:, \"continent_2\"])\n",
    "        continent_df.loc[:, \"same_cont\"] = continent_df.loc[:, \"same_cont\"].replace({True:1, False:0})\n",
    "        \n",
    "        return continent_df.loc[:, \"same_cont\"]\n",
    "    \n",
    "    # Read file with access token.\n",
    "    # To do: error handling\n",
    "    def set_token(self, src):\n",
    "        if src != None:\n",
    "            with open(src + \"_Access_Token.txt\", \"r\") as f:\n",
    "                self._access_token = f.readline()\n",
    "\n",
    "            \n",
    "    def set_url(self, url):\n",
    "        self._url = url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Child class to download financial data from stocks etc.\n",
    "class financialData(macroData):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(financialData, self).__init__(**kwargs)\n",
    "        \n",
    "        # Fill correlation matrix upon initialization\n",
    "        self._corr_matrix = self._populate_corr_matrix()    \n",
    "            \n",
    "            \n",
    "    # Calculate correlations for each year that is in \n",
    "    # the specified range upon initialization\n",
    "    def _populate_corr_matrix(self):\n",
    "        ix_dict = self._lookup_df.loc[:, [\"FX\", \"IX\"]].set_index(\"FX\")\n",
    "        ix_dict = ix_dict.to_dict()[\"IX\"]\n",
    "        \n",
    "        df = pd.concat([self.get_stock(symbol=ix_dict[c], \n",
    "                                       resample=None).rename(columns={ix_dict[c]:c})\n",
    "                        for c in self._cset], axis=1)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df.loc[:, \"Year\"] = df.index.year\n",
    "        corr_df = df.groupby(\"Year\").corr()\n",
    "        \n",
    "        corr_matrix = {}\n",
    "        for y in df.loc[:, \"Year\"].values:\n",
    "            corr_matrix[y] = corr_df.query(\"Year == \" + str(y)).loc[y]\n",
    "            \n",
    "        return corr_matrix\n",
    "    \n",
    "    # Return correlation matrix for a given year\n",
    "    def get_corr_matrix(self, year=None):\n",
    "        return self._corr_matrix.get(year, \"Enter valid year\")\n",
    "    \n",
    "    \n",
    "    # Return time series of financial instrument identified by\n",
    "    # symbol and source like iex, morningstar, NASDAQ...\n",
    "    # By default, resample quarterly and return last.\n",
    "    def get_stock(self, symbol=None, source=None, \n",
    "                  start=None, end=None, resample=\"Q\"):\n",
    "        \n",
    "        # Default value handling.\n",
    "        if symbol == None:\n",
    "            symbol = \"SPY\"\n",
    "        if source == None:\n",
    "            source = \"morningstar\"\n",
    "        if start == None:\n",
    "            start = self._st0\n",
    "        if end == None:\n",
    "            end = self._st1\n",
    "            \n",
    "        df = data.DataReader(symbol, source, start, end)\n",
    "        \n",
    "        df = df.reset_index()\n",
    "        df = df.loc[df[\"Volume\"] > 0, [\"Date\", \"Close\"]].rename(columns={\"Close\":symbol})\n",
    "        df.loc[:, \"Date\"] = pd.to_datetime(df.loc[:, \"Date\"])\n",
    "        df = df.set_index(\"Date\")\n",
    "        \n",
    "        if resample == None:\n",
    "            return df\n",
    "        else:\n",
    "            return df.resample(resample).last()\n",
    "        \n",
    "        \n",
    "    # Return oil prices provided by eia.gov\n",
    "    # By default, resample quarterly and return last.\n",
    "    def get_oil(self, start=None, end=None, resample=\"Q\"):\n",
    "        \n",
    "        if start == None:\n",
    "            start = self._st0\n",
    "        if end == None:\n",
    "            end = self._st1\n",
    "            \n",
    "        oil_df = pd.read_excel(\"http://www.eia.gov/dnav/pet/hist_xls/RCLC1d.xls\", \n",
    "                               sheet_name=1, header=2, index_col=0, parse_dates=True)\n",
    "        oil_df.columns = [\"Oil_Price\"]\n",
    "        \n",
    "        return oil_df.loc[start:end].resample(resample).last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tradeData(macroData):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(tradeData, self).__init__(**kwargs)\n",
    "        self.set_url(\"http://comtrade.un.org/api/get\")\n",
    "        self.set_token(None)\n",
    "        \n",
    "        self._trade_matrix = {}\n",
    "        self._check_local_files(self._t0.year, self._t1.year)\n",
    "        \n",
    "        \n",
    "    def _check_local_files(self, y0, y1):\n",
    "        missing_yr = []\n",
    "        for yr in range(y0, y1+1):\n",
    "            try:\n",
    "                self._trade_matrix[yr] = pd.read_csv(\"data/trade_matrix_\"+str(yr)+\".csv\",\n",
    "                                                     index_col=0)\n",
    "            except:\n",
    "                missing_yr.append(str(yr))\n",
    "        \n",
    "        if len(missing_yr) == 0:\n",
    "            print \"Trade matrices complete.\"\n",
    "        else:\n",
    "            print \"Trade matrices missing: \", \", \".join(missing_yr)\n",
    "            \n",
    "            \n",
    "    def _year_string(self, y0, y1):\n",
    "        return [\",\".join(map(str, year_range)) \n",
    "                for year_range \n",
    "                in np.array_split(np.arange(y0,y1+1), (y1-y0)/3 + 1)]\n",
    "    \n",
    "    \n",
    "    def _timing_wrapper(func):\n",
    "        from time import sleep\n",
    "        \n",
    "        def wrapper(*args, **kwargs):\n",
    "            sleep()\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    \n",
    "    \n",
    "    @_timing_wrapper\n",
    "    def _request(self, **params):\n",
    "        content = super(tradeData, self)._request(**params)\n",
    "        return content\n",
    "    \n",
    "    \n",
    "    def _pull_data(self, **params):\n",
    "        params[\"fmt\"] = \"json\"\n",
    "        params[\"cc\"]  = \"TOTAL\"\n",
    "        \n",
    "        content = self._request(**params)\n",
    "        assert type(content) == dict, \"Data is not dict\\n\" + content\n",
    "        content = content[\"dataset\"]\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(content)\n",
    "        if len(df) > 0:\n",
    "            df = df.loc[(df[\"rgDesc\"] == \"Import\") | (df[\"rgDesc\"] == \"Export\")]\n",
    "            df = df.loc[:, [\"yr\", \"rgDesc\", \"rt3ISO\", \"pt3ISO\", \"TradeValue\"]]  \n",
    "            df.loc[:, \"yr\"] = pd.to_datetime(df.loc[:, \"yr\"], format=\"%Y\")\n",
    "            df.loc[:, \"TradeValue\"] = df.loc[:, \"TradeValue\"].apply(np.int)\n",
    "            df = df.rename(columns={\"yr\": \"Year\", \"rgDesc\": \"Kind\",\n",
    "                                    \"rt3ISO\": \"Reporter\", \"pt3ISO\": \"Partner\"})\n",
    "            return df.set_index(\"Year\")\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    def get_country_trade(self, country, **params):\n",
    "        y0, y1 = self._t0.year, self._t1.year\n",
    "        lookup_df = self._lookup_df.dropna()\n",
    "        lookup_df = lookup_df.loc[lookup_df[\"FX\"].isin(self._cset)]\n",
    "\n",
    "        un_codes = lookup_df.loc[lookup_df[\"FX\"] == country, \"UN\"].values\n",
    "        \n",
    "        dl_years = self._year_string(y0, y1)\n",
    "        \n",
    "        df = pd.concat([\n",
    "            pd.concat([self._pull_data(r=str(reporter), p=\"All\", ps=y) \n",
    "                       for y in dl_years])\n",
    "            for reporter in un_codes])\n",
    "        \n",
    "        df.loc[:, \"Reporter\"] = df.loc[:, \"Reporter\"].map(self._iso_dict[\"FX\"])\n",
    "        df.loc[:, \"Partner\"] = df.loc[:, \"Partner\"].map(self._iso_dict[\"FX\"])\n",
    "        df = df.loc[df[\"Partner\"].isin(self._cset)].reset_index()\n",
    "        df = df.groupby([\"Year\", \"Reporter\", \n",
    "                         \"Kind\", \"Partner\"]).sum().drop(country, level=3)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def get_trade_matrices(self, year=None):\n",
    "        return self._trade_matrix.get(year, \"Enter valid year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class econoData(macroData):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(econoData, self).__init__(**kwargs)\n",
    "        self.set_url(\"https://api.stlouisfed.org/fred\")\n",
    "        self.set_token(\"FRED\")\n",
    "\n",
    "        \n",
    "    def _add_keywords(self, **params):\n",
    "        params[\"endpoint\"] = \"series/observations\"\n",
    "        params[\"file_type\"] = \"json\"\n",
    "        params[\"api_key\"] = self._access_token\n",
    "        \n",
    "        if \"start\" in params:\n",
    "            time = params.pop(\"start\")\n",
    "            params[\"realtime_start\"] = time\n",
    "        if \"end\" in params:\n",
    "            time = params.pop(\"end\")\n",
    "            params[\"realtime_end\"] = time\n",
    "            \n",
    "        return params\n",
    "        \n",
    "    \n",
    "    def _pull_data(self, **params):\n",
    "        start = params.get(\"start\", self._st0)\n",
    "        end = params.get(\"end\", \"9999-12-31\")\n",
    "        \n",
    "        params = self._add_keywords(**params)\n",
    "        \n",
    "        content = self._request(**params)\n",
    "        content = content[\"observations\"]\n",
    "        \n",
    "        df = pd.DataFrame.from_dict(content)\n",
    "        \n",
    "        assert len(df) > 0, \"No data for \" + params[\"series\"].upper()\n",
    "        df = df.loc[df[\"date\"] >= start, [\"date\", \"value\"]].set_index(\"date\")\n",
    "        \n",
    "        return self._format_float_df(df)\n",
    "    \n",
    "    \n",
    "    def get_series(self, series=None, country=None, **params):\n",
    "        def api_str(series, country):\n",
    "            series_dict = {\"GDP\": (\"MKTGDP\", \"A646NWDB\"),\n",
    "                           \"10YR\": (\"IRLTLT01\", \"A156N\"), \n",
    "                           \"UNEMP\": (\"LRHUTTTT\", \"A156N\")}\n",
    "            return series_dict[series][0] + country + series_dict[series][1]\n",
    "            \n",
    "        if series == None:\n",
    "            series = \"10YR\"\n",
    "        if country == None:\n",
    "            country = \"US\"\n",
    "        \n",
    "        series, country = series.upper(), country.upper()\n",
    "        \n",
    "        params[\"series_id\"] = api_str(series, country)\n",
    "            \n",
    "        df = self._pull_data(**params)\n",
    "        \n",
    "        return df.rename(columns={\"value\":country+\"_\"+series})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade matrices complete.\n"
     ]
    }
   ],
   "source": [
    "ed = econoData(cset=curr, t0=\"20120101\", t1=\"20171231\")\n",
    "cs = currencySet(df, symbols=symbols, t0=\"20120101\", t1=\"20171231\")\n",
    "fd = financialData(cset=curr, t0=\"20120101\", t1=\"20171231\")\n",
    "ct = tradeData(cset=curr, t0=\"20120101\", t1=\"20171231\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kld_dict = {}\n",
    "for y in range(2012,2018):\n",
    "    cs.make_kld(export=True, year=y)\n",
    "    kld_dict[y] = cs.get_kld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kld(year, export=False):\n",
    "    fig, ax = plt.subplots(figsize=(6.5,5))\n",
    "    plot_df = kld_dict[year]\n",
    "    sns.heatmap(plot_df, cmap=\"Reds\", vmax=0.4, square=True, ax=ax)\n",
    "    ax.set_xlabel(\"Currency\", fontdict={\"size\":18})\n",
    "    ax.set_ylabel(\"Currency\", fontdict={\"size\":18})\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation='vertical', fontsize=12);\n",
    "    plt.setp(ax.get_yticklabels(), rotation='horizontal', fontsize=12);\n",
    "    ax.set_title(\"KL-Divergence in \" + str(year),fontsize=20);\n",
    "\n",
    "    plt.tight_layout();\n",
    "    if export:\n",
    "        fig.savefig(\"fig/KLD_annual_\"+ str(year)+ \".png\", bbox_inches=\"tight\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in range(2012,2018):\n",
    "    plot_kld(y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interest rates / Fisher effect\n",
    "# Purchasing power\n",
    "# 10 year yields\n",
    "# GDP\n",
    "# Unemployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
